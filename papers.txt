Rotary Position Embedding for Vision Transformer. - noget med position encoding vi m√•ske kan bruge.
https://arxiv.org/html/2403.13298v1

An image is worth 16x16 words: Transformers for image recognition at scale. - transformer til billeder, original paper.
https://arxiv.org/abs/2010.11929

Example on vit
https://medium.com/thedeephub/building-vision-transformer-from-scratch-using-pytorch-an-image-worth-16x16-words-24db5f159e27

Example on vit
https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html#Transformers-for-image-classification

On attention visualization
https://www.youtube.com/watch?v=7q3NGMkEtjI

Overview of ViT explainability methods
https://jacobgil.github.io/deeplearning/vision-transformer-explainability
https://github.com/jacobgil/vit-explain/blob/main/vit_rollout.py

Attention rollout
https://arxiv.org/pdf/2005.00928
https://medium.com/@nivonl/exploring-visual-attention-in-transformer-models-ab538c06083a

Transformer explainability beyond attention visualization
https://arxiv.org/abs/2012.09838
https://medium.com/orohealth/explainable-ai-using-vision-transformers-on-skin-disease-images-9148d1583faf
