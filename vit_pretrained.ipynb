{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import lightning as L\n",
    "from torchvision.models import vit_b_16, vit_b_32 # pretrained model\n",
    "import torchmetrics\n",
    "import cv2\n",
    "import optuna\n",
    "import albumentations as A\n",
    "\n",
    "import brain_tumor_dataset as btd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Augmentation**               | **Why Use It?**                                                                                           |\n",
    "|---------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| **Rotate**                     | Simulates orientation differences.                                                                       |\n",
    "| **Horizontal**\t\t\t\t | Increases diversity while retaining symmetry.                                                           |\n",
    "| **Random Crop**                | Focuses on smaller regions of interest.                                                                 |\n",
    "| **Elastic Transform**          | Introduces non-rigid variations.                                                                        |\n",
    "| **Brightness/Contrast Adjustment** | Mimics scanner variability.                                                                             |\n",
    "| **CLAHE**                      | Enhances local contrast, highlighting subtle features.                                                  |\n",
    "| **Gaussian Noise**             | Simulates noisy scans.                                                                                  |\n",
    "| **Compression**                | Mimics lossy storage artifacts.                                                                         |\n",
    "| **Coarse Dropout**             | Simulates signal loss or occluded areas.                                                                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = A.Compose([\n",
    "    # Geometric transformations\n",
    "    A.Rotate(limit=30., border_mode=cv2.BORDER_CONSTANT, p=0.4), # Simulates slight differences in patient orientation during the scan.\n",
    "    A.HorizontalFlip(p=0.6),  # Random horizontal flip\n",
    "\tA.ElasticTransform(alpha=1.0, sigma=50.0, p=0.2), # simulates the deformation of the brain tissue during the scan\n",
    "\n",
    "\t# intensity and contrast transformations\n",
    "    A.RandomBrightnessContrast(p=0.4),  # Mimics variations in scanner settings or patient-specific image intensity\n",
    "    A.CLAHE(p=.3),  # Contrast Limited Adaptive Histogram Equalization (to enhance local contrast)\n",
    "    A.RandomGamma(p=.3),  # Random gamma adjustment for exposure variance\n",
    "    \n",
    "\t# Noise and artifacts\n",
    "    A.GaussNoise(p=.4),  # Simulates random noise in the image from the scanning process\n",
    "    A.ImageCompression(quality_range=(60, 95), p=0.2),  # Simulates image compression artifacts\n",
    "    A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.2),  # Simulates missing parts of the image\n",
    "])\n",
    "\n",
    "train_dataset = btd.BrainTumorDataset(btd.TRAIN_DATA_PATH)\n",
    "btd.show_augmentations(train_dataset[0][0], augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btd.augment_data(augmentations=augmentations,\n",
    "                 file_path=btd.TRAIN_DATA_PATH,\n",
    "                 num_augmentations=10, # TODO: REDUCE\n",
    "                 overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataModule(L.LightningDataModule):\n",
    "\tdef __init__(self, batch_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\tdef setup(self, stage=None):\n",
    "\t\ttransform = transforms.Compose([\n",
    "\t\t\ttransforms.Grayscale(num_output_channels=3),   # convert to 3 channels\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\tbtd.CropImgTransform(),                \t\t   # crop the image\n",
    "\t\t\ttransforms.Resize((224, 224)),                 # resize to 224x224\n",
    "\t\t])\n",
    "\n",
    "\t\tself.train_dataset = btd.BrainTumorDataset(btd.TRAIN_DATA_PATH, transform=transform)\n",
    "\t\tself.test_dataset = btd.BrainTumorDataset(btd.TEST_DATA_PATH, transform=transform)\n",
    "\n",
    "\t\tval_size = len(self.test_dataset) // 2\n",
    "\t\ttest_size = len(self.test_dataset) - val_size\n",
    "\t\tself.test_dataset, self.val_dataset = torch.utils.data.random_split(self.test_dataset, [test_size, val_size])\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\treturn torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\treturn torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "\tdef test_dataloader(self):\n",
    "\t\treturn torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "dm = BrainTumorDataModule(1)\n",
    "dm.setup()\n",
    "\n",
    "# see len of datasets\n",
    "print(len(dm.train_dataset), len(dm.val_dataset), len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the classifier (LightningModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch Lightning Module\n",
    "class BrainTumorClassifier(L.LightningModule):\n",
    "\tdef __init__(self, \n",
    "\t\t\t  \tlearning_rate=1e-4, \n",
    "\t\t\t\tweight_decay=0.01,\n",
    "\t\t\t\tpatch_size=16,\n",
    "\t\t\t  \tpretrained_weights = \"IMAGENET1K_V1\",\n",
    "\t\t\t\tbatch_size=-1): # -1 not specified and only for logging\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.save_hyperparameters()  # Save hyperparameters for easy access\n",
    "\n",
    "\t\t# Initialize the model with the pre-trained ViT\n",
    "\t\tif patch_size == 16:\n",
    "\t\t\tself.model = vit_b_16(weights=pretrained_weights)\n",
    "\t\telif patch_size == 32:\n",
    "\t\t\tself.model = vit_b_32(weights=pretrained_weights)\n",
    "\t\t\t\n",
    "\t\tself.model.heads = torch.nn.Linear(self.model.hidden_dim, 4)  # Modify for 4 classes\n",
    "\n",
    "\t\t# Define loss function\n",
    "\t\tself.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\t\t# Initialize accuracy metric for logging\n",
    "\t\tself.train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
    "\t\tself.val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
    "\n",
    "\t\tself.attention_maps = {}  # To store attention maps\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\tinputs, labels = batch\n",
    "\t\toutputs = self(inputs)\n",
    "\t\tloss = self.criterion(outputs, labels)\n",
    "\n",
    "\t\t# Log loss and accuracy\n",
    "\t\tself.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\t\tself.train_accuracy(outputs, labels)\n",
    "\t\tself.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\tinputs, labels = batch\n",
    "\t\toutputs = self(inputs)\n",
    "\t\tloss = self.criterion(outputs, labels)\n",
    "\n",
    "\t\t# Log loss and accuracy\n",
    "\t\tself.log('val_loss', loss)\n",
    "\t\tself.val_accuracy(outputs, labels)\n",
    "\t\tself.log('val_acc', self.val_accuracy)\n",
    "\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef test_step(self, batch, batch_idx):\n",
    "\t\tinputs, labels = batch\n",
    "\t\toutputs = self(inputs)\n",
    "\t\tloss = self.criterion(outputs, labels)\n",
    "\n",
    "\t\t# Log loss and accuracy\n",
    "\t\tself.log('test_loss', loss)\n",
    "\t\tself.val_accuracy(outputs, labels)\n",
    "\t\tself.log('test_acc', self.val_accuracy)\n",
    "\n",
    "\t\tself.log_attention_maps(inputs, labels, outputs, batch_idx)  # Save attention maps\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)\n",
    "\t\n",
    "\tdef register_attention_hooks(self):\n",
    "\t\t\"\"\"Register hooks to capture attention maps.\"\"\"\n",
    "\t\tself.attention_maps.clear()  # Reset attention maps\n",
    "\n",
    "\t\tdef hook_fn(module, input, output, module_name):\n",
    "\t\t\tmodule_name = module_name.split(\".\")[2]  # Get the layer name\n",
    "\t\t\t# inside vision_transformer.py, change the forward function of EncoderBlock to \n",
    "\t\t\t# x, _ = self.self_attention(x, x, x, need_weights=True) | and set need_weights=True not False\n",
    "\t\t\tself.attention_maps[module_name] = output[1]  # Save the attention map\n",
    "\n",
    "\t\t# Register hooks on all MultiheadAttention layers\n",
    "\t\tfor i, module in self.model.named_modules():\n",
    "\t\t\tif isinstance(module, torch.nn.MultiheadAttention):\n",
    "\t\t\t\tmodule.register_forward_hook(lambda module, input, output, module_name=i: hook_fn(module, input, output, module_name))\n",
    "\t\n",
    "\tdef on_test_start(self):\n",
    "\t\t# Register the hook to each multi-head attention layer before testing\n",
    "\t\tself.register_attention_hooks()\n",
    "\n",
    "\tdef log_attention_maps(self, inputs, labels, output, batch_idx):\n",
    "\t\t\t\"\"\"Log attention maps overlaid on the original image using Lightning's logger.\"\"\"\n",
    "\t\t\t\n",
    "\t\t\tpredicted_labels = [btd.BrainTumorDataset().idx_to_class[lbl.item()] for lbl in torch.argmax(output, dim=1)]\n",
    "\t\t\tbatch_size = inputs.size(0)\n",
    "\t\t\tfor i in range(batch_size):\n",
    "\t\t\t\trollout_attention_map = torch.eye(self.attention_maps['encoder_layer_1'].size(-1))\n",
    "\t\t\t\tfor _, attention in self.attention_maps.items():\n",
    "\t\t\t\t\t# Get the attention map for the first image in the batch\n",
    "\t\t\t\t\tattention_map = attention[i]  # Shape: [num_tokens, embedding_size]\n",
    "     \n",
    "\t\t\t\t\tattention_map = attention_map + torch.eye(attention_map.size(-1))  # Add identity matrix\n",
    "\t\t\t\t\tattention_map /= attention_map.sum(dim=-1, keepdim=True)  # Normalize attention map\n",
    "\t\t\t\t\tif rollout_attention_map is None:\n",
    "\t\t\t\t\t\trollout_attention_map = attention_map\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\trollout_attention_map = torch.matmul(rollout_attention_map, attention_map)\n",
    "\n",
    "\t\t\t\t# Compute cosine similarity between class token and patches\n",
    "\t\t\t\t# class_token_embedding = class_token_embedding / torch.norm(class_token_embedding)\n",
    "\t\t\t\tnum_patches_side = int((attention_map.size(0) - 1) ** 0.5)\n",
    "\t\t\t\tclass_token_embedding = rollout_attention_map[0, 1:] # Shape: (embedding_size)\n",
    "\t\t\t\tattention_map = 1 - class_token_embedding.view(num_patches_side, num_patches_side, -1).clone()\n",
    "\t\t\t\tattention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
    "\t\t\t\t# average_attention_map = average_attention_map / num_layers\n",
    "\n",
    "\t\t\t\taverage_attention_map = attention_map.cpu().detach().numpy()\n",
    "\t\t\t\taverage_attention_map = cv2.resize(average_attention_map, (inputs.size(2), inputs.size(3)))\n",
    "\t\t\t\theatmap = cv2.applyColorMap(np.uint8(255 * average_attention_map), cv2.COLORMAP_JET)\n",
    "\n",
    "\t\t\t\t# Overlay the heatmap on the original image\n",
    "\t\t\t\timage = inputs[i].cpu().numpy().transpose(1, 2, 0)\n",
    "\t\t\t\timage = (image*255).astype(np.uint8)\n",
    "\t\t\t\toverlayed_image = cv2.addWeighted(image, 0.8, heatmap, 0.4, 0)\n",
    "\n",
    "\t\t\t\t# add class label\n",
    "\t\t\t\tlabel = labels[i].item()\n",
    "\t\t\t\tlabel = btd.BrainTumorDataset().idx_to_class[label]\n",
    "\t\t\t\tcv2.putText(overlayed_image, f\"Real class: {label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\t\t\t\tcv2.putText(overlayed_image, f\"Predicted class: {predicted_labels[i]}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\t\t\t\t# convert to chw\n",
    "\t\t\t\toverlayed_image = overlayed_image.transpose(2, 0, 1)\n",
    "\n",
    "\t\t\t\t# log to tensorboard\n",
    "\t\t\t\tself.logger.experiment.add_image(f'attn_map/batch_{batch_idx}/img_{i}', overlayed_image, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (using Optuna) \n",
    "Based on [this guide](https://medium.com/swlh/optuna-hyperparameter-optimization-in-pytorch-9ab5a5a39e77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna (the function to optimize)\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    patch_size = trial.suggest_categorical('patch_size', [16, 32])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "\n",
    "    # Create the model and data module with suggested hyperparameters\n",
    "    model = BrainTumorClassifier(learning_rate=learning_rate, \n",
    "                                 weight_decay=weight_decay,\n",
    "                                 patch_size=patch_size,\n",
    "                                 batch_size=batch_size)\n",
    "    data_module = BrainTumorDataModule(batch_size=batch_size)\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"epoch-{epoch:02d}-val_loss-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = L.pytorch.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        mode=\"min\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    logger = L.pytorch.loggers.TensorBoardLogger(\"logs\", name=\"vit_pretrained\")\n",
    "\n",
    "    # Define the PyTorch Lightning Trainer\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"auto\",\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    # report intermediate objective value\n",
    "    trial.report(val_loss, step=trainer.current_epoch)\n",
    "\n",
    "    # Handle pruning based on the intermediate value\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"vit_pretrained\")\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True, n_jobs=-1)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# trainer.test(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
