{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andreas\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import lightning as L\n",
    "from torchvision.models import vit_b_16 # pretrained model\n",
    "import torchmetrics\n",
    "import cv2\n",
    "import optuna\n",
    "import albumentations as A\n",
    "\n",
    "import brain_tumor_dataset as btd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas\\AppData\\Local\\Temp\\ipykernel_27672\\1687396370.py:18: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n",
      "  A.ElasticTransform(alpha=1.0, sigma=50.0, alpha_affine=50.0, p=0.2),  # Elastic transform for non-rigid deformations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 existing augmented images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in C:\\Users\\Andreas\\.cache\\kagglehub\\datasets\\masoudnickparvar\\brain-tumor-mri-dataset\\versions\\1\\Training/: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented 0 images in C:\\Users\\Andreas\\.cache\\kagglehub\\datasets\\masoudnickparvar\\brain-tumor-mri-dataset\\versions\\1\\Training/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images in C:\\Users\\Andreas\\.cache\\kagglehub\\datasets\\masoudnickparvar\\brain-tumor-mri-dataset\\versions\\1\\Training/glioma:  13%|█▎        | 169/1321 [01:08<07:48,  2.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m      1\u001b[0m augmentations \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Geometric transformations\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     A\u001b[38;5;241m.\u001b[39mRotate(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),  \u001b[38;5;66;03m# Rotate with a limit of 30 degrees\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     A\u001b[38;5;241m.\u001b[39mRandomGamma(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),  \u001b[38;5;66;03m# Random gamma adjustment for exposure variance\u001b[39;00m\n\u001b[0;32m     20\u001b[0m ])\n\u001b[1;32m---> 22\u001b[0m btd\u001b[38;5;241m.\u001b[39maugment_data(augmentations\u001b[38;5;241m=\u001b[39maugmentations,\n\u001b[0;32m     23\u001b[0m                  file_path\u001b[38;5;241m=\u001b[39mbtd\u001b[38;5;241m.\u001b[39mTRAIN_DATA_PATH,\n\u001b[0;32m     24\u001b[0m                  num_augmentations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     25\u001b[0m                  overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Andreas\\Documents\\GitHub\\DL24Project\\brain_tumor_dataset.py:179\u001b[0m, in \u001b[0;36maugment_data\u001b[1;34m(augmentations, file_path, num_augmentations, overwrite)\u001b[0m\n\u001b[0;32m    177\u001b[0m         aug_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(file_name)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_aug\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfile_extension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         aug_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, aug_file_name)\n\u001b[1;32m--> 179\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimwrite(aug_file_path, cv2\u001b[38;5;241m.\u001b[39mcvtColor(augmented_img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[0;32m    180\u001b[0m         augmented_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAugmented \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maugmented_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "augmentations = A.Compose([\n",
    "    # Geometric transformations\n",
    "    A.Rotate(limit=30, p=0.5),  # Rotate with a limit of 30 degrees\n",
    "    A.HorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    A.VerticalFlip(p=0.5),  # Random vertical flip\n",
    "\n",
    "    # Intensity and contrast adjustments\n",
    "    A.RandomBrightnessContrast(p=0.2),  # Random brightness and contrast adjustment\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.2),  # CLAHE for better contrast\n",
    "\n",
    "    # Noise \n",
    "    A.GaussNoise(var_limit=(10, 50), p=0.3),  # Add Gaussian noise\n",
    "    \n",
    "\t# image compression artifacts\n",
    "    A.ImageCompression(quality_range=(50, 90), p=0.3),  # Add JPEG compression artifacts\n",
    "\n",
    "    # Other transformations\n",
    "    A.ElasticTransform(alpha=1.0, sigma=50.0, alpha_affine=50.0, p=0.2),  # Elastic transform for non-rigid deformations\n",
    "    A.RandomGamma(p=0.2),  # Random gamma adjustment for exposure variance\n",
    "])\n",
    "\n",
    "btd.augment_data(augmentations=augmentations,\n",
    "                 file_path=btd.TRAIN_DATA_PATH,\n",
    "                 num_augmentations=20,\n",
    "                 overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorDataModule(L.LightningDataModule):\n",
    "\tdef __init__(self, batch_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\tdef setup(self, stage=None):\n",
    "\t\ttransform = transforms.Compose([\n",
    "\t\t\ttransforms.Grayscale(num_output_channels=3),   # convert to 3 channels\n",
    "\t\t\ttransforms.ToTensor(),\n",
    "\t\t\tbtd.CropImgTransform(),                \t\t   # crop the image\n",
    "\t\t\ttransforms.Resize((224, 224)),                 # resize to 224x224\n",
    "\t\t\t\n",
    "\t\t])\n",
    "\n",
    "\t\tself.train_dataset = btd.BrainTumorDataset(btd.TRAIN_DATA_PATH, transform=transform)\n",
    "\t\tself.test_dataset = btd.BrainTumorDataset(btd.TEST_DATA_PATH, transform=transform)\n",
    "\n",
    "\t\tval_size = len(self.test_dataset) // 2\n",
    "\t\ttest_size = len(self.test_dataset) - val_size\n",
    "\t\tself.test_dataset, self.val_dataset = torch.utils.data.random_split(self.test_dataset, [test_size, val_size])\n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\treturn torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\treturn torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "\tdef test_dataloader(self):\n",
    "\t\treturn torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9097 655 656\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "dm = BrainTumorDataModule(1)\n",
    "dm.setup()\n",
    "\n",
    "# see len of datasets\n",
    "print(len(dm.train_dataset), len(dm.val_dataset), len(dm.test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch Lightning Module\n",
    "class BrainTumorClassifier(L.LightningModule):\n",
    "\tdef __init__(self, \n",
    "\t\t\t  \tlearning_rate=1e-4, \n",
    "\t\t\t\tweight_decay=0.01,\n",
    "\t\t\t  \tpretrained_weights = \"IMAGENET1K_V1\"):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.save_hyperparameters()  # Save hyperparameters for easy access\n",
    "\n",
    "\t\t# Initialize the model with the pre-trained ViT\n",
    "\t\tself.model = vit_b_16(weights=pretrained_weights)\n",
    "\t\tself.model.heads = torch.nn.Linear(self.model.hidden_dim, 4)  # Modify for 4 classes\n",
    "\n",
    "\t\t# Define loss function\n",
    "\t\tself.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\t\t# Initialize accuracy metric for logging\n",
    "\t\tself.train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
    "\t\tself.val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
    "\n",
    "\t\tself.attention_maps = {}  # To store attention maps\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x)\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\tinputs, labels = batch\n",
    "\t\toutputs = self(inputs)\n",
    "\t\tloss = self.criterion(outputs, labels)\n",
    "\n",
    "\t\t# Log loss and accuracy\n",
    "\t\tself.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\t\tself.train_accuracy(outputs, labels)\n",
    "\t\tself.log('train_acc', self.train_accuracy, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\tinputs, labels = batch\n",
    "\t\toutputs = self(inputs)\n",
    "\t\tloss = self.criterion(outputs, labels)\n",
    "\n",
    "\t\t# Log loss and accuracy\n",
    "\t\tself.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "\t\tself.val_accuracy(outputs, labels)\n",
    "\t\tself.log('val_acc', self.val_accuracy, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef test_step(self, batch, batch_idx):\n",
    "\t\tinputs, labels = batch\n",
    "\t\toutputs = self(inputs)\n",
    "\t\tloss = self.criterion(outputs, labels)\n",
    "\n",
    "\t\t# Log loss and accuracy\n",
    "\t\tself.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "\t\tself.val_accuracy(outputs, labels)\n",
    "\t\tself.log('test_acc', self.val_accuracy, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\t\tself.log_attention_maps(inputs, labels, outputs, batch_idx)  # Save attention maps\n",
    "\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay)\n",
    "\t\n",
    "\tdef register_attention_hooks(self):\n",
    "\t\t\"\"\"Register hooks to capture attention maps.\"\"\"\n",
    "\t\tself.attention_maps.clear()  # Reset attention maps\n",
    "\n",
    "\t\tdef hook_fn(module, input, output, module_name):\n",
    "\t\t\tmodule_name = module_name.split(\".\")[2]  # Get the layer name\n",
    "\t\t\t# inside vision_transformer.py, change the forward function of EncoderBlock to \n",
    "\t\t\t# x, _ = self.self_attention(x, x, x, need_weights=True) | and set need_weights=True not False\n",
    "\t\t\tself.attention_maps[module_name] = output[1]  # Save the attention map\n",
    "\n",
    "\t\t# Register hooks on all MultiheadAttention layers\n",
    "\t\tfor i, module in self.model.named_modules():\n",
    "\t\t\tif isinstance(module, torch.nn.MultiheadAttention):\n",
    "\t\t\t\tmodule.register_forward_hook(lambda module, input, output, module_name=i: hook_fn(module, input, output, module_name))\n",
    "\t\n",
    "\tdef on_test_start(self):\n",
    "\t\t# Register the hook to each multi-head attention layer before testing\n",
    "\t\tself.register_attention_hooks()\n",
    "\n",
    "\tdef log_attention_maps(self, inputs, labels, output, batch_idx):\n",
    "\t\t\t\"\"\"Log attention maps overlaid on the original image using Lightning's logger.\"\"\"\n",
    "\t\t\t\n",
    "\t\t\tpredicted_labels = [btd.BrainTumorDataset().idx_to_class[lbl.item()] for lbl in torch.argmax(output, dim=1)]\n",
    "\t\t\tbatch_size = inputs.size(0)\n",
    "\t\t\tfor i in range(batch_size):\n",
    "\t\t\t\trollout_attention_map = torch.eye(self.attention_maps['encoder_layer_1'].size(-1))\n",
    "\t\t\t\tfor _, attention in self.attention_maps.items():\n",
    "\t\t\t\t\t# Get the attention map for the first image in the batch\n",
    "\t\t\t\t\tattention_map = attention[i]  # Shape: [num_tokens, embedding_size]\n",
    "     \n",
    "\t\t\t\t\tattention_map = attention_map + torch.eye(attention_map.size(-1))  # Add identity matrix\n",
    "\t\t\t\t\tattention_map /= attention_map.sum(dim=-1, keepdim=True)  # Normalize attention map\n",
    "\t\t\t\t\tif rollout_attention_map is None:\n",
    "\t\t\t\t\t\trollout_attention_map = attention_map\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\trollout_attention_map = torch.matmul(rollout_attention_map, attention_map)\n",
    "\n",
    "\t\t\t\t# Compute cosine similarity between class token and patches\n",
    "\t\t\t\t# class_token_embedding = class_token_embedding / torch.norm(class_token_embedding)\n",
    "\t\t\t\tnum_patches_side = int((attention_map.size(0) - 1) ** 0.5)\n",
    "\t\t\t\tclass_token_embedding = rollout_attention_map[0, 1:] # Shape: (embedding_size)\n",
    "\t\t\t\tattention_map = 1 - class_token_embedding.view(num_patches_side, num_patches_side, -1).clone()\n",
    "\t\t\t\tattention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
    "\t\t\t\t# average_attention_map = average_attention_map / num_layers\n",
    "\n",
    "\t\t\t\taverage_attention_map = attention_map.cpu().detach().numpy()\n",
    "\t\t\t\taverage_attention_map = cv2.resize(average_attention_map, (inputs.size(2), inputs.size(3)))\n",
    "\t\t\t\theatmap = cv2.applyColorMap(np.uint8(255 * average_attention_map), cv2.COLORMAP_JET)\n",
    "\n",
    "\t\t\t\t# Overlay the heatmap on the original image\n",
    "\t\t\t\timage = inputs[i].cpu().numpy().transpose(1, 2, 0)\n",
    "\t\t\t\timage = (image*255).astype(np.uint8)\n",
    "\t\t\t\toverlayed_image = cv2.addWeighted(image, 0.8, heatmap, 0.4, 0)\n",
    "\n",
    "\t\t\t\t# add class label\n",
    "\t\t\t\tlabel = labels[i].item()\n",
    "\t\t\t\tlabel = btd.BrainTumorDataset().idx_to_class[label]\n",
    "\t\t\t\tcv2.putText(overlayed_image, f\"Real class: {label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\t\t\t\tcv2.putText(overlayed_image, f\"Predicted class: {predicted_labels[i]}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\t\t\t\t# convert to chw\n",
    "\t\t\t\toverlayed_image = overlayed_image.transpose(2, 0, 1)\n",
    "\n",
    "\t\t\t\t# log to tensorboard\n",
    "\t\t\t\tself.logger.experiment.add_image(f'attn_map/batch_{batch_idx}/img_{i}', overlayed_image, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna \n",
    "Using [this guide](https://medium.com/swlh/optuna-hyperparameter-optimization-in-pytorch-9ab5a5a39e77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna op\n",
    "\n",
    "# Define the objective function for Optuna (the function to optimize)\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    # Create the model and data module with suggested hyperparameters\n",
    "    model = BrainTumorClassifier(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    data_module = BrainTumorDataModule(batch_size=batch_size)\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint_callback = L.pytorch.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=\"checkpoints/\",\n",
    "        filename=\"epoch-{epoch:02d}-val_loss-{val_loss:.2f}\",\n",
    "        save_top_k=1,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = L.pytorch.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        mode=\"min\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    logger = L.pytorch.loggers.TensorBoardLogger(\"logs\", name=\"vit_pretrained\")\n",
    "\n",
    "    # Define the PyTorch Lightning Trainer\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"auto\",\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "    # report intermediate objective value\n",
    "    trial.report(val_loss, step=trainer.current_epoch)\n",
    "\n",
    "    # Handle pruning based on the intermediate value\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"vit_pretrained\")\n",
    "study.optimize(objective, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 21/21 [01:38<00:00,  0.21it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9908536672592163\n",
      "        test_loss          0.034989044070243835\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.034989044070243835, 'test_acc': 0.9908536672592163}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model\n",
    "# trainer.test(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
